# self-attention
 Python implementation of the scaled-dot product attention mechanism originally proposed in "Attention Is All You Need".
